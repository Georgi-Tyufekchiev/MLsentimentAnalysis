{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6iMbqPnJatt"
      },
      "source": [
        "Text Classification Project"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install transformers"
      ],
      "metadata": {
        "id": "XFji1ouhne1O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Necessary Code\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import KFold, cross_val_score,cross_val_predict\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from statistics import mean\n",
        "from numpy import std\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "df = pd.read_table(\"text.txt\",header=None)\n",
        "df.columns = ['text']\n",
        "df[['topic','sentiment','id','text']]= df.text.str.split(' ',n=3,expand=True)\n",
        "\n",
        "\n",
        "df = df.drop_duplicates(subset=['text'], keep='first')\n",
        "print(df.sentiment.value_counts())\n",
        "\n",
        "# pre-processing\n",
        "df['text'] = df['text'].str.replace(\" n't \", \" not \")\n",
        "df['text'] = df['text'].str.replace(\" 's \", \"'s \")\n",
        "df['text'] = df['text'].str.replace(\" 'm \", \"'m \")\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "contractions = {\n",
        "    \"n't\": \"not\",\n",
        "    \"'re\": \"are\",\n",
        "    \"'s\": \"\",\n",
        "    \"'m\": \"\",\n",
        "    \"'ve\": \"have\",\n",
        "    \"'ll\": \"will\",\n",
        "    \"'d\": \"would\"\n",
        "}\n",
        "pattern = re.compile(\"|\".join(contractions.keys()))\n",
        "\n",
        "stop_words = set(nltk.corpus.stopwords.words(\"english\"))\n",
        "punctuation = set(string.punctuation)\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "df['text'] = df['text'].apply(lambda x: ' '.join([lemmatizer.lemmatize(word.lower())\n",
        "for word in word_tokenize(pattern.sub(lambda match: contractions[match.group(0)], x))\n",
        "if word.lower() not in stop_words and word not in punctuation]))\n",
        "\n",
        "# df.to_csv('/files/text_new.csv', index=False)\n",
        "\n",
        "mapper = {\"pos\": 0, \"neg\": 1}\n",
        "df['sentiment'] = df.sentiment.map(mapper)\n",
        "\n"
      ],
      "metadata": {
        "id": "DuIB-aMZ55ZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ML MODELS"
      ],
      "metadata": {
        "id": "A9l-kJwCN1gz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Glove"
      ],
      "metadata": {
        "id": "LPcDLPt-HdlB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import pad_sequences\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.model_selection import KFold\n",
        "from tensorflow.python.keras.layers import Dense, Embedding, LSTM, Dropout\n",
        "from tensorflow.python.keras.models import Sequential\n",
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "from tensorflow.python.keras.regularizers import l2\n",
        "def get_weight_matrix(vocab, vocab_size, embeddings_index):\n",
        "    weight_matrix = np.zeros((vocab_size, 100))\n",
        "    for word, i in vocab.items():\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            # words not found in embedding index will be all-zeros.\n",
        "            weight_matrix[i] = embedding_vector\n",
        "\n",
        "    return weight_matrix\n",
        "\n",
        "\n",
        "embeddings_index = {}\n",
        "with open('glove.6B.100d.txt', encoding='utf8') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "print('GloVe data loaded')\n",
        "df = pd.read_csv('text_new.csv', encoding='utf-8')\n",
        "\n",
        "df['sentiment'] = df['sentiment'].map({'neg': 1, 'pos': 0})\n",
        "\n",
        "X = df.text.values\n",
        "y = df['sentiment']\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X)\n",
        "X = tokenizer.texts_to_sequences(X)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "X = pad_sequences(X, maxlen=200)\n",
        "embedding_vectors = get_weight_matrix(tokenizer.word_index, vocab_size, embeddings_index)\n",
        "mirrored_strategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:0\"])\n",
        "with mirrored_strategy.scope():\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(vocab_size, output_dim=100, input_length=200,\n",
        "                        trainable=False))\n",
        "    model.set_weights([embedding_vectors])\n",
        "    model.add(LSTM(units=128))\n",
        "    model.add(Dropout(0.1))\n",
        "    model.add(Dense(1, activation='sigmoid', kernel_regularizer=l2(0.0001)))\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "\n",
        "kfold = KFold(n_splits=5, shuffle=True)\n",
        "for train_indices, val_indices in kfold.split(X):\n",
        "    X_train, X_val = X[train_indices], X[val_indices]\n",
        "    y_train, y_val = y[train_indices], y[val_indices]\n",
        "    model.fit(X_train, y_train,\n",
        "              batch_size=128,\n",
        "              epochs=7,\n",
        "              verbose=1,\n",
        "              validation_data=(X_val, y_val))\n",
        "\n",
        "    y_pred = (model.predict(X_val) >= 0.5).astype(\"int\")\n",
        "\n",
        "    score = accuracy_score(y_val, y_pred)\n",
        "    cm = confusion_matrix(y_val, y_pred)\n",
        "\n",
        "\n",
        "    print(classification_report(y_val, y_pred))\n",
        "    print(f'Accuracy score: {score:.4f}')\n",
        "    print(f'Confusion matrix:\\n{cm}')\n"
      ],
      "metadata": {
        "id": "OY2K3k-6Het2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "W2V"
      ],
      "metadata": {
        "id": "b3h_LkrgHfJq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize():\n",
        "    X = []\n",
        "    stop_words = set(nltk.corpus.stopwords.words(\"english\"))\n",
        "    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
        "    for par in news[\"text\"].values:\n",
        "        tmp = []\n",
        "        sentences = nltk.sent_tokenize(par)\n",
        "        for sent in sentences:\n",
        "            sent = sent.lower()\n",
        "            tokens = tokenizer.tokenize(sent)\n",
        "            filtered_words = [w.strip() for w in tokens if w not in stop_words and len(w) > 1]\n",
        "            tmp.extend(filtered_words)\n",
        "        X.append(tmp)\n",
        "\n",
        "    return X\n",
        "\n",
        "def get_weight_matrix(model, vocab, vocab_size):\n",
        "    weight_matrix = np.zeros((vocab_size, 100))\n",
        "    # step vocab, store vectors using the Tokenizer's integer mapping\n",
        "    for word, i in vocab.items():\n",
        "        if word in model.wv.key_to_index:\n",
        "            weight_matrix[i] = model.wv.get_vector(word)\n",
        "        else:\n",
        "            print(i)\n",
        "    return weight_matrix\n",
        "\n",
        "\n",
        "df = pd.read_csv('text_new.csv', encoding='utf-8')\n",
        "df['sentiment'] = df['sentiment'].map({'neg': 1, 'pos': 0})\n",
        "\n",
        "X = normalize()\n",
        "y = df['sentiment']\n",
        "\n",
        "w2v = gensim.models.Word2Vec(X, vector_size=100, window=10, min_count=1)\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X)\n",
        "X = tokenizer.texts_to_sequences(X)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "X = pad_sequences(X, maxlen=200)\n",
        "embedding_vectors = get_weight_matrix(w2v, tokenizer.word_index, vocab_size)\n",
        "mirrored_strategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:0\"])\n",
        "with mirrored_strategy.scope():\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(vocab_size, output_dim=100, input_length=200,\n",
        "                        trainable=False))\n",
        "    model.set_weights([embedding_vectors])\n",
        "    model.add(LSTM(units=128))\n",
        "    model.add(Dropout(0.1))\n",
        "    model.add(Dense(1, activation='sigmoid', kernel_regularizer=l2(0.00001)))\n",
        "\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "\n",
        "kfold = KFold(n_splits=5, shuffle=True)\n",
        "for train_indices, val_indices in kfold.split(X):\n",
        "    X_train, X_val = X[train_indices], X[val_indices]\n",
        "    y_train, y_val = y[train_indices], y[val_indices]\n",
        "    model.fit(X_train, y_train,\n",
        "              batch_size=128,\n",
        "              epochs=8,\n",
        "              verbose=1,\n",
        "              validation_data=(X_val, y_val))\n",
        "\n",
        "    y_pred = (model.predict(X_val) >= 0.5).astype(\"int\")\n",
        "\n",
        "    # Calculate the accuracy score\n",
        "    score = accuracy_score(y_val, y_pred)\n",
        "    cm = confusion_matrix(y_val, y_pred)\n",
        "\n",
        "    print(classification_report(y_val, y_pred))\n",
        "\n",
        "    # Print the accuracy score\n",
        "    print(f'Accuracy score: {score:.4f}')\n",
        "    print(f'Confusion matrix:\\n{cm}')\n",
        "\n"
      ],
      "metadata": {
        "id": "Vf_zZQ1zHgNu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic Regression"
      ],
      "metadata": {
        "id": "Vtf8kj_iN5T5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.text.values.astype(\"str\")\n",
        "X = TfidfVectorizer().fit_transform(X)\n",
        "\n",
        "y = df.sentiment.values\n",
        "\n",
        "model = LogisticRegression()\n",
        "\n",
        "for k in range(2,15):\n",
        "  cv = KFold(n_splits=k, random_state=1, shuffle=True)\n",
        "  scoreF1 = cross_val_score(model, X, y, scoring=\"f1\", cv=cv, n_jobs=-1)\n",
        "  scoreRecall = cross_val_score(model, X, y, scoring=\"recall\", cv=cv, n_jobs=-1)\n",
        "  scorePrecision = cross_val_score(model, X, y, scoring=\"precision\", cv=cv, n_jobs=-1)\n",
        "  scoreAccuracy = cross_val_score(model, X, y, scoring=\"accuracy\", cv=cv, n_jobs=-1)\n",
        "\n",
        "  print('F1 for k %d: %.3f (%.3f)' % (k, mean(scoreF1), std(scoreF1)))\n",
        "  print('Recall for k %d: %.3f (%.3f)' % (k, mean(scoreRecall), std(scoreRecall)))\n",
        "  print('Precision for k %d: %.3f (%.3f)' % (k, mean(scorePrecision), std(scorePrecision)))\n",
        "  print('Accuracy for k %d: %.3f (%.3f)' % (k, mean(scoreAccuracy), std(scoreAccuracy)))\n",
        "  # Use cross_val_predict to make predictions for each fold of the cross-validation\n",
        "  y_pred = cross_val_predict(model, X, y, cv=cv, n_jobs=-1)\n",
        "\n",
        "  # Compute the confusion matrix for the combined predictions\n",
        "  cm = confusion_matrix(y, y_pred)\n",
        "\n",
        "  print(cm)\n",
        "  print()\n"
      ],
      "metadata": {
        "id": "NSygGLZN1Wg4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Naive Bayes"
      ],
      "metadata": {
        "id": "tN1Bwzy91dju"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Read csv file\n",
        "news=pd.read_csv('text_new.csv')\n",
        "news['sentiment'].replace(['neg', 'pos'], [0,1], inplace=True)\n",
        "x_train, x_test, y_train, y_test = train_test_split(news[\"text\"], news[\"sentiment\"], test_size=0.2, stratify=news[\"sentiment\"], random_state=1)\n",
        "news.head()\n",
        "\n",
        "# Pre-processing & pipeline\n",
        "model_nb = Pipeline([\n",
        "    (\"tf-idf\",TfidfVectorizer(stop_words=\"english\")),\n",
        "    (\"clf\",MultinomialNB())\n",
        "])\n",
        "\n",
        "# k = 5-fold cross-validation\n",
        "scores_nb = cross_validate(model_nb, x_train, y_train, cv=5, scoring=(\"accuracy\", \"f1\", \"recall\", \"precision\"), n_jobs=5)\n",
        "\n",
        "# train the model on the entire training set\n",
        "model_nb.fit(x_train, y_train)\n",
        "\n",
        "print(\"Mean accuracy:\", scores_nb[\"test_accuracy\"].mean())\n",
        "print(\"Accuracy standard deviation:\", scores_nb[\"test_accuracy\"].std())\n",
        "print(\"Mean f1 score:\", scores_nb[\"test_f1\"].mean())\n",
        "print(\"Mean recall score:\", scores_nb[\"test_recall\"].mean())\n",
        "print(\"Mean precision score:\", scores_nb[\"test_precision\"].mean())\n",
        "# print(\"conusion matrix : \", confusion_matrix(y_test,scores_nb) )"
      ],
      "metadata": {
        "id": "FiXagtae1dCe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "KNN"
      ],
      "metadata": {
        "id": "qxAzlpKb1nzv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read csv file\n",
        "\n",
        "scores_knn = cross_validate(model_knn, x_train, y_train, cv=5, scoring=(\"accuracy\", \"f1\", \"recall\", \"precision\"))\n",
        "\n",
        "error_rate = []\n",
        "\n",
        "\n",
        "for i in range(1,100):\n",
        "    model_knn = Pipeline([\n",
        "        (\"tf-idf\", TfidfVectorizer(stop_words=\"english\")),\n",
        "        (\"clf\", KNeighborsClassifier(n_neighbors=i))\n",
        "    ])\n",
        "    model_knn.fit(x_train, y_train)\n",
        "    y_test_predicted = model_knn.predict(x_test)\n",
        "    error_rate.append(np.mean(y_test_predicted != y_test))\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.plot(range(1,40),error_rate,color='blue', linestyle='dashed', marker='o',\n",
        "         markerfacecolor='red', markersize=10)\n",
        "plt.title('Error Rate vs. K Value')\n",
        "plt.xlabel('K')\n",
        "plt.ylabel('Error Rate')\n",
        "\n",
        "\n",
        "news=pd.read_csv('text_new.csv')\n",
        "news['sentiment'].replace(['neg', 'pos'], [0,1], inplace=True)\n",
        "x_train, x_test, y_train, y_test = train_test_split(news[\"text\"], news[\"sentiment\"], test_size=0.2, stratify=news[\"sentiment\"], random_state=1)\n",
        "news.head()\n",
        "\n",
        "# Pre-processing & pipeline\n",
        "model_KNN = Pipeline([\n",
        "    (\"tf-idf\",TfidfVectorizer(stop_words=\"english\")),\n",
        "    (\"clf\", KNeighborsClassifier())\n",
        "])\n",
        "\n",
        "# k = 5-fold cross-validation\n",
        "for k in range(2,15):\n",
        "  scores_KNN = cross_validate(model_KNN, x_train, y_train, cv=k, scoring=(\"accuracy\", \"f1\", \"recall\", \"precision\"), n_jobs=5)\n",
        "\n",
        "  # train the model on the entire training set\n",
        "  model_KNN.fit(x_train, y_train)\n",
        "  print(\"KFOLD-%d\"%k)\n",
        "  print(\"Mean accuracy:\", scores_KNN[\"test_accuracy\"].mean())\n",
        "  print(\"Accuracy standard deviation:\", scores_KNN[\"test_accuracy\"].std())\n",
        "  print(\"Mean f1 score:\", scores_KNN[\"test_f1\"].mean())\n",
        "  print(\"Mean recall score:\", scores_KNN[\"test_recall\"].mean())\n",
        "  print(\"Mean precision score:\", scores_KNN[\"test_precision\"].mean())\n",
        "  print()"
      ],
      "metadata": {
        "id": "AdHIlgiG1pJs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Duplicate IDs\n",
        "id_counts = df['id'].value_counts()\n",
        "duplicate_ids = id_counts[id_counts > 1]\n",
        "duplicate_id_list = []\n",
        "max_occurence = (None,0)\n",
        "min_occurence = (None,10**5)\n",
        "freq = {}\n",
        "for id, count in zip(duplicate_ids.index, duplicate_ids.values):\n",
        "    duplicate_id_list.append((id, count))\n",
        "    if count > max_occurence[1]:\n",
        "      max_occurence = (id,count)\n",
        "    if count < min_occurence[1]:\n",
        "      min_occurence = (id,count)\n",
        "    if count in freq:\n",
        "      freq[count] +=1\n",
        "    else:\n",
        "      freq[count] = 1\n",
        "\n",
        "print(\"Amount of IDs with duplicates: %d\" % len(duplicate_id_list))\n",
        "print(\"ID with most duplicates: %s-%d\" % max_occurence) #582.txt\n",
        "print(\"ID with fewest duplicates: %s-%d\" % min_occurence) # 957.txt\n",
        "# for duplicates, count in freq.items():\n",
        "#   print(\"This amount of duplicates - %d - occurs %d times\" % (duplicates,count))\n",
        "\n",
        "id_mask = df.id == '582.txt'\n",
        "id_rows = df.loc[id_mask]\n",
        "print(id_rows)\n",
        "\n",
        "# Duplicate text\n",
        "\n",
        "duplicates_id_text = df.duplicated(subset=['id', 'text'], keep=False)\n",
        "num_duplicates = duplicates_id_text.sum()\n",
        "print(f'There are {num_duplicates} rows with the same ID and text.')\n",
        "\n",
        "\n",
        "duplicates_text = df.duplicated(subset=['text'], keep=False)\n",
        "num_duplicates = duplicates_text.sum()\n",
        "print(f'There are {num_duplicates} rows with the same text.')\n",
        "duplicate_text = df[duplicates_text]\n",
        "\n",
        "print(duplicate_text.sort_values(by=['text']))\n",
        "\n",
        "cleaned = df.drop_duplicates(subset=['text'], keep='first')\n",
        "duplicates_text = cleaned.duplicated(subset=['text'], keep=False)\n",
        "num_duplicates = duplicates_text.sum()\n",
        "print(f'There are {num_duplicates} rows with the same text.')\n",
        "\n",
        "\n",
        "# Task 7: Draw Wordcloud\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Source for tutorial: https://www.the-analytics.club/word-cloud-python-example\n",
        "\n",
        "# Step 1: Separate dataframe into a pos and neg dataframe\n",
        "pos_df = df[df['sentiment'] == 'pos']\n",
        "neg_df = df[df['sentiment'] == 'neg']\n",
        "\n",
        "# Step 2: Concatenate all positive and negative reviews into 1 string (all_pos_text, all_neg_text)\n",
        "all_pos_text = pos_df['text'].str.cat(sep=' ')\n",
        "all_neg_text = neg_df['text'].str.cat(sep=' ')\n",
        "\n",
        "# Step 2.5: Remove all words that are smaller than 3 characters\n",
        "# Use the method spilt to get each word, then use list comprehension to append new word into new list\n",
        "all_pos_text_list = [word for word in all_pos_text.split() if len(word) > 3]\n",
        "all_neg_text_list = [word for word in all_neg_text.split() if len(word) > 3]\n",
        "\n",
        "# Use join method to convert list into a single string that will be fed to the WordCloud\n",
        "# Note that ' ' separates the words in the string\n",
        "all_pos_text_cleaned = ' '.join(all_pos_text_list)\n",
        "all_neg_text_cleaned = ' '.join(all_neg_text_list)\n",
        "\n",
        "# Step 3: Generate wordcloud\n",
        "pos_wordcloud = WordCloud().generate(all_pos_text_cleaned)\n",
        "neg_wordcloud = WordCloud().generate(all_neg_text_cleaned)\n",
        "\n",
        "\n",
        "# Step 4: Display generated wordclouds\n",
        "plt.imshow(pos_wordcloud, interpolation = 'bilinear')\n",
        "plt.axis('off')\n",
        "plt.title('Positive Reviews', color = 'green', fontdict = {'fontsize': 24}) # Red green for positive review & Make font larger\n",
        "plt.figure(figsize=(380,380))\n",
        "plt.show()\n",
        "\n",
        "plt.imshow(neg_wordcloud, interpolation = 'bilinear')\n",
        "plt.axis('off')\n",
        "plt.title('Negative Reviews', color = 'red', fontdict = {'fontsize': 24}) # Red colour for negative review & Make font larger\n",
        "plt.figure(figsize=(380,380))\n",
        "plt.show()\n",
        "\n",
        "import re\n",
        "\n",
        "df['text_len_chars'] = df['text'].str.len()\n",
        "df['text_len_words'] = df['text'].apply(lambda x: len(re.findall(r'\\b\\w+\\b', x)))\n",
        "\n",
        "print(\"Length of texts segment: \\n\")\n",
        "# calculate the average length of texts in characters and words\n",
        "avg_len_chars = df['text_len_chars'].mean()\n",
        "avg_len_words = df['text_len_words'].mean()\n",
        "print(\"Average length of texts (in characters): {:.2f}\".format(avg_len_chars))\n",
        "print(\"Average length of texts (in words): {:.2f} \".format(avg_len_words))\n",
        "print()\n",
        "\n",
        "# calculate the average word count for positive and negative reviews\n",
        "avg_word_count_pos = df.loc[df['sentiment'] == 'pos', 'text_len_words'].mean()\n",
        "avg_word_count_neg = df.loc[df['sentiment'] == 'neg', 'text_len_words'].mean()\n",
        "\n",
        "print(\"Average word count for positive reviews:\", round(avg_word_count_pos, 2))\n",
        "print(\"Average word count for negative reviews:\", round(avg_word_count_neg, 2))\n",
        "print()\n",
        "\n",
        "# calculate the average word length of texts for each category\n",
        "avg_word_len_by_category = df.groupby('topic')['text_len_words'].mean()\n",
        "print(\"Average word length of texts for each category:\")\n",
        "print(avg_word_len_by_category)\n",
        "print()\n",
        "\n",
        "# calculate the variance of text lengths in characters and words\n",
        "var_words = df['text_len_words'].var()\n",
        "print(\"Variance of text lengths (in words): {:.2f}\".format(var_words))\n",
        "print()\n",
        "\n",
        "# find the smallest and longest texts\n",
        "min_idx = df['text_len_chars'].idxmin()\n",
        "max_idx = df['text_len_chars'].idxmax()\n",
        "\n",
        "smallest_text = df.loc[min_idx, 'text']\n",
        "smallest_text_topic = df.loc[min_idx, 'topic']\n",
        "smallest_text_sentiment = df.loc[min_idx, 'sentiment']\n",
        "smallest_text_word_len = df.loc[min_idx, 'text_len_words']\n",
        "\n",
        "longest_text = df.loc[max_idx, 'text']\n",
        "longest_text_topic = df.loc[max_idx, 'topic']\n",
        "longest_text_sentiment = df.loc[max_idx, 'sentiment']\n",
        "longest_text_word_len = df.loc[max_idx, 'text_len_words']\n",
        "\n",
        "print(\"Smallest text:\")\n",
        "print(\"Topic:\", smallest_text_topic)\n",
        "print(\"Sentiment:\", smallest_text_sentiment)\n",
        "print(\"Text:\", smallest_text)\n",
        "print(\"Word length:\", smallest_text_word_len)\n",
        "print(\"\\nLongest text:\")\n",
        "print(\"Topic:\", longest_text_topic)\n",
        "print(\"Sentiment:\", longest_text_sentiment)\n",
        "print(\"Text:\", longest_text)\n",
        "print(\"Word length:\", longest_text_word_len)\n",
        "\n",
        "\n",
        "###### SENTIMENT CLASSES DISTRIBUTION\n",
        "camera_sentiment_dist=df[df[\"topic\"] == \"camera\"].sentiment.value_counts()\n",
        "music_sentiment_dist=df[df[\"topic\"] == \"music\"].sentiment.value_counts()\n",
        "health_sentiment_dist=df[df[\"topic\"] == \"health\"].sentiment.value_counts()\n",
        "dvd_sentiment_dist=df[df[\"topic\"] == \"dvd\"].sentiment.value_counts()\n",
        "books_sentiment_dist=df[df[\"topic\"] == \"books\"].sentiment.value_counts()\n",
        "software_sentiment_dist=df[df[\"topic\"] == \"software\"].sentiment.value_counts()\n",
        "\n",
        "fig, axes = plt.subplots(nrows=2, ncols=3)\n",
        "\n",
        "camera_sentiment_dist.plot(kind='pie', autopct='%1.0f%%', figsize=(15,12), title=\"camera\", ax=axes[0,0])\n",
        "music_sentiment_dist.plot(kind='pie', autopct='%1.0f%%', figsize=(15,12), title=\"music\", ax=axes[0,1])\n",
        "health_sentiment_dist.plot(kind='pie', autopct='%1.0f%%', figsize=(15,12), title=\"health\", ax=axes[0,2])\n",
        "dvd_sentiment_dist.plot(kind='pie', autopct='%1.0f%%', figsize=(15,12), title=\"dvd\", ax=axes[1,0])\n",
        "books_sentiment_dist.plot(kind='pie', autopct='%1.0f%%', figsize=(15,12), title=\"books\", ax=axes[1,1])\n",
        "software_sentiment_dist.plot(kind='pie', autopct='%1.0f%%', figsize=(15,12), title=\"software\", ax=axes[1,2])\n",
        "\n",
        "############## BINARIZE DATA\n",
        "neg_df=df[df[\"sentiment\"] == \"neg\"]\n",
        "pos_df=df[df[\"sentiment\"] == \"pos\"]\n",
        "\n",
        "################ LABEL ENCODE\n",
        "# df[\"topic\"] = df[\"topic\"].astype(int)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UoGxRUfw6YoR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 1\n",
        "# Show an example from the dataset\n",
        "# read it as table, read_csv breaks it.\n",
        "df = pd.read_table(\"text.txt\",header=None)\n",
        "# extract an example entry\n",
        "print(df.head(1))\n",
        "# Task 2\n",
        "# Transform the data into a form that we can work with\n",
        "# set a column for the dataframe\n",
        "df.columns = ['text']\n",
        "# split the text into columns\n",
        "df[['topic','sentiment','id','text']]= df.text.str.split(' ',n=3,expand=True)\n",
        "print(df.head(3))\n",
        "# Task 3\n",
        "# Get the distribution of positive and negative reviews\n",
        "print(df.sentiment.value_counts())\n",
        "# Task 4: How about the distribution of topics? Are the topics normally distributed?\n",
        "topic_dist=df.topic.value_counts()\n",
        "topic_dist.plot(kind='pie', autopct='%1.0f%%', figsize=(15,12))\n",
        "# Task 5: How about the distribution of sentiment among topics?\n",
        "for topic in df.topic.unique():\n",
        "  print(\"Topic %s has %d negative reviews\" % (topic,len(df[(df['topic']==str(topic)) & (df['sentiment']=='neg')])))\n",
        "  print(\"Topic %s has %d positive reviews\" % (topic,len(df[(df['topic']==str(topic)) & (df['sentiment']=='pos')])))\n",
        "  # Task 6: How about the length of texts? Are the texts in similar length? Any large or short sentences that you can drop from your study?\n",
        "import re\n",
        "\n",
        "df['text_len_chars'] = df['text'].str.len()\n",
        "df['text_len_words'] = df['text'].apply(lambda x: len(re.findall(r'\\b\\w+\\b', x)))\n",
        "\n",
        "# calculate the average length of texts in characters and words\n",
        "avg_len_chars = df['text_len_chars'].mean()\n",
        "avg_len_words = df['text_len_words'].mean()\n",
        "print(\"Average length of texts (in characters): {:.2f}\".format(avg_len_chars))\n",
        "print(\"Average length of texts (in words): {:.2f} \".format(avg_len_words))\n",
        "print(\"\")\n",
        "# calculate the average word length of texts for each category\n",
        "avg_word_len_by_category = df.groupby('topic')['text_len_words'].mean()\n",
        "print(\"Average word length of texts for each category:\")\n",
        "print(avg_word_len_by_category)\n",
        "print()\n",
        "\n",
        "# calculate the variance of text lengths in characters and words\n",
        "var_words = df['text_len_words'].var()\n",
        "print(\"Variance of text lengths (in words): {:.2f}\".format(var_words))\n",
        "print()\n",
        "\n",
        "# find the smallest and longest texts\n",
        "smallest_text = df.loc[df['text_len_chars'].idxmin(), 'text']\n",
        "smallest_text_word_len = df.loc[df['text_len_chars'].idxmin(), 'text_len_words']\n",
        "longest_text = df.loc[df['text_len_chars'].idxmax(), 'text']\n",
        "longest_text_word_len = df.loc[df['text_len_chars'].idxmax(), 'text_len_words']\n",
        "\n",
        "print(\"Smallest text:\\n\", smallest_text)\n",
        "print(\"Word length of smallest text:\", smallest_text_word_len)\n",
        "print(\"\\nLongest text:\\n\", longest_text)\n",
        "print(\"Word length of longest text:\", longest_text_word_len)\n",
        "\n",
        "# Task 7: Draw Wordcloud\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Source for tutorial: https://www.the-analytics.club/word-cloud-python-example\n",
        "\n",
        "# Step 1: Separate dataframe into a pos and neg dataframe\n",
        "pos_df = df[df['sentiment'] == 'pos']\n",
        "neg_df = df[df['sentiment'] == 'neg']\n",
        "\n",
        "# Step 2: Concatenate all positive and negative reviews into 1 string (all_pos_text, all_neg_text)\n",
        "all_pos_text = pos_df['text'].str.cat(sep=' ')\n",
        "all_neg_text = neg_df['text'].str.cat(sep=' ')\n",
        "\n",
        "# Step 3: Generate wordcloud\n",
        "pos_wordcloud = WordCloud().generate(all_pos_text)\n",
        "neg_wordcloud = WordCloud().generate(all_neg_text)\n",
        "\n",
        "# Step 4: Display generated wordclouds\n",
        "plt.imshow(pos_wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title('Positive Reviews')\n",
        "plt.figure(figsize=(380,380))\n",
        "plt.show()\n",
        "\n",
        "plt.imshow(neg_wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title('Negative Reviews')\n",
        "plt.figure(figsize=(380,380))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NVq_rdUa6Fwh"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}